{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "import matplotlib.pyplot as plt\n",
    "##\n",
    "import json\n",
    "import re\n",
    "##\n",
    "import numpy           as np\n",
    "import pandas          as pd\n",
    "import statsmodels.api as sm\n",
    "##\n",
    "from elasticsearch     import Elasticsearch, helpers\n",
    "from elasticsearch_dsl import Search, A, Q\n",
    "\n",
    "##\n",
    "import thundermint.plot  as plot\n",
    "import thundermint.splot as splot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"credentials.json\") as f:\n",
    "    credentials = json.load(f)\n",
    "es = Elasticsearch(\"%s:%s@elastic.hxr.team\" % (credentials['login'], credentials['password']), \n",
    "                   port=443, \n",
    "                   use_ssl=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 'xenochain-2018-11-08'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Primitive filters\n",
    "\n",
    "def flt_host(host) :\n",
    "    \"filter by cluster\"\n",
    "    return lambda s : s.filter('term', host=host)\n",
    "def flt_cluster(cluster) :\n",
    "    \"filter by cluster\"\n",
    "    return lambda s : s.filter('term', env=cluster)\n",
    "def flt_severity(sev) :\n",
    "    \"Filter by severity\"\n",
    "    return lambda s : s.filter('term',  sev=sev)\n",
    "def flt_namespace(ns) :\n",
    "    \"Filter on namespace\"\n",
    "    return lambda s : s.filter('term',  ns=ns)\n",
    "\n",
    "flt_consensus = flt_namespace('consensus')\n",
    "flt_mempool   = flt_namespace('mempool')\n",
    "flt_net       = flt_namespace('net')\n",
    "\n",
    "def flt_time(start, delta):\n",
    "    \"Filter time range\"\n",
    "    (h1,m1) = start\n",
    "    (dh,dm) = delta\n",
    "    h2 = h1 + dh + (m1+dm) // 60\n",
    "    m2 = (m1+dm) % 60   \n",
    "    r  = re.match(\"^.*-(\\d+-\\d+-\\d+)\", index)\n",
    "    t1 = \"%sT%02i:%02i:00Z\" % (r.group(1), h1, m1)\n",
    "    t2 = \"%sT%02i:%02i:00Z\" % (r.group(1), h2, m2)\n",
    "    return lambda s : s.filter('range', at ={\"gte\":t1, \"lt\":t2})\n",
    "\n",
    "def make_seatch(cluster, filters) :\n",
    "    \"Create Search object from filters\"\n",
    "    s = Search(using=es, index=index)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Postprocessing\n",
    "\n",
    "def postprocess_entries(s) :\n",
    "    r       = pd.DataFrame.from_records([x.to_dict() for x in s.scan()])\n",
    "    r['at'] = pd.to_datetime(r['at'])\n",
    "    r       = r.sort_values('at')\n",
    "    return r\n",
    "\n",
    "def postprocess_mempool(r):\n",
    "    r['size']      = r['data'].apply(lambda x: x['size'])\n",
    "    r['filtered']  = r['data'].apply(lambda x: x['filtered'])\n",
    "    r['added']     = r['data'].apply(lambda x: x['added'])\n",
    "    r['discarded'] = r['data'].apply(lambda x: x['discarded'])\n",
    "    r = r.drop('data', axis=1)\n",
    "    return r\n",
    "\n",
    "def split_on_host(df):\n",
    "    return {k : d.reset_index() for k,d in df.groupby('host')}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Queries\n",
    "\n",
    "def q_agg_uniq(cluster, filters, field):\n",
    "    \"Aggregate by unique field\"\n",
    "    s = make_seatch(cluster, filters)\n",
    "    s.aggs.bucket('unique_ids', A('terms', field=field))\n",
    "    return s[0:0].execute().aggregations.unique_ids.buckets\n",
    "\n",
    "def q_scan(cluster, filters, host=None, source=['at','msg','data','host'], postprocess=[]):\n",
    "    \"Fetch data from elastic\"\n",
    "    s = make_seatch(cluster, filters).source(source)\n",
    "    s = flt_cluster(cluster)(s)\n",
    "    if host is not None:\n",
    "        s = flt_host(host)(s)\n",
    "    r = postprocess_entries(s)\n",
    "    for f in postprocess:\n",
    "        r = f(r)\n",
    "    if host is None:\n",
    "        r = split_on_host(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_fields(field):\n",
    "    \"Count distinct field values in current index\"\n",
    "    s = Search(using=es, index=index)[0:0]\n",
    "    s.aggs.bucket('unique_ids', A('terms', field=field))\n",
    "    return s.execute().aggregations.unique_ids.buckets\n",
    "\n",
    "def query_consensus(start, delta, cluster):\n",
    "    \"Fetch data for logs about consensus\"\n",
    "    (t1,t2) = time_range(start, delta)\n",
    "    s = Search(using=es, index=index).\\\n",
    "        filter('term',  env=cluster).\\\n",
    "        filter('term',  ns ='consensus').\\\n",
    "        filter('range', at ={\"gte\":t1, \"lt\":t2}).\\\n",
    "        source(['at','msg','data','host'])\n",
    "    return split_on_host(postprocess_entries(s))\n",
    "\n",
    "def query_mempool(start, delta, cluster):\n",
    "     # Query\n",
    "    (t1,t2) = time_range(start, delta)\n",
    "    s = Search(using=es, index=index).\\\n",
    "        filter('term',  env=cluster).\\\n",
    "        filter('term',  ns ='mempool').\\\n",
    "        filter('range', at ={\"gte\":t1, \"lt\":t2}).\\\n",
    "        source(['at','msg','data','host'])\n",
    "    r = postprocess_entries(s)\n",
    "    r['size']      = r['data'].apply(lambda x: x['size'])\n",
    "    r['filtered']  = r['data'].apply(lambda x: x['filtered'])\n",
    "    r['added']     = r['data'].apply(lambda x: x['added'])\n",
    "    r['discarded'] = r['data'].apply(lambda x: x['discarded'])\n",
    "    r = r.drop('data', axis=1)\n",
    "    return split_on_host(r)\n",
    "\n",
    "def query_errors(cluster) :\n",
    "    s = Search(using=es, index=index).\\\n",
    "        filter('term', env=cluster).\\\n",
    "        filter('term', sev='Error').\\\n",
    "        source(['at','msg','data','host'])\n",
    "    r = postprocess_entries(s)\n",
    "    return split_on_host(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsC = q_scan('atum', [flt_namespace('consensus'),\n",
    "                        flt_time((11,0), (0,5))])\n",
    "logsM = q_scan('atum', [flt_namespace('mempool'),\n",
    "                        flt_time((11,0), (0,5))],\n",
    "                postprocess=[postprocess_mempool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_commit_time(logsC)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_n_tx_in_block(logsC)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_mempool_size([x for x in logsM.values()])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splot.splot(logsC, w=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = logsC['validator5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=d.drop(['index','host'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(t1,t2) = time_range((10,5),(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Search(using=es, index=index).\\\n",
    "        filter('term',  env='atum').\\\n",
    "        filter('term',  ns ='net').\\\n",
    "        filter('range', at ={\"gte\":t1, \"lt\":t2}).\\\n",
    "        filter('term', host='validator5').\\\n",
    "        source(['at','msg','data','sev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = postprocess_entries(s).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[df['msg']=='Gossip stats'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame.from_records(df1['data'].values)\n",
    "df2['at'] = df1['at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.grid()\n",
    "plt.plot(df2['at'].values, df2['RxPC'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llc = query_consensus((10,5),(0,20), 'atum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= llc['validator5'].drop(['host'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lambda n : d[n:n+50])(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
